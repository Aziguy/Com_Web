{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapping du Contexte d’utilisation\n",
    "\n",
    "Afin de détailler précisément les informations que nous avons, nous souhaitons connaitre le contexte d’utilisation de l’hyperlien vers le site du Parc National.\n",
    "\n",
    "S’agit-il d’une citation dans un texte ? Est-il dans une liste ? La liste est non exhaustive.\n",
    "\n",
    "## Définition des librairies\n",
    "\n",
    "Pour ce dernier script, nous aurons besoin d’un total de 6 librairies :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#************************[ Librairies Utilisées ]************************************************************************************\n",
    "\n",
    "import pandas as pd #Gestion des Dataframes & Import/ Export Csv\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup #Librairie scraping\n",
    "import re # Permet la recherche partielle sur les pages\n",
    "import time #pause du script pour évite blacklisting de l'adresse IP\n",
    "\n",
    "from tqdm import tqdm #Affichage d'une barre de progression du script\n",
    "tqdm.pandas()  #permet l'utilisation de la fonction pd.progress_apply pour voir la progression du script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ``Pandas `` nous permettra de gérer le volume important de données présentes dans les trois fichiers CSV que nous avons comme source.\n",
    "* ``Requests``, nous permettra de récupérer le contenu des pages web recensées.\n",
    "* ``Bs4 ``, est la librairie qui nous permettra de traiter le contexte sur les pages récupérées.\n",
    "* ``re``, qui nous permettra d’effectuer une recherche partielle sur la page\n",
    "* ``time ``, nous permettra d’éviter un blacklisting potentiel afin de pouvoir continuer à accéder aux sites comme bandol-blog.com qui posséde un grand nombre de pages.\n",
    "* ``tdqm ``, qui nous permettra d’activer la fonction progress_apply avec la librairie Pandas. Cette dernière nous permettra d’avoir un visuel de la vitesse de traitement des données.\n",
    "\n",
    "\n",
    "## Les fonctions de ce script\n",
    "\n",
    "\n",
    "Ce script fonctionnera grâce à la fonction ``Scrapper``.\n",
    "Cette dernière récupérera deux informations : \n",
    "- `Referring` (l’adresse de la page de l’acteur)\n",
    "- `Cible` (le nom de domaine utilisé sur la page de l’acteur)\n",
    "\n",
    "À partir de ces deux informations, la fonction grâce à la librairie requests va enregistrer le contenu de la page Referring.\n",
    "La librairie ``Bs4 `` sera ensuite utilisée, et plus précisément sa fonction `BeautifulSoup` qui va lire le contenu et le traiter, ce qui va nous permettre de rechercher dans la page les balises HTML \"a\" (qui contiennent les hyperliens).\n",
    "\n",
    "Puis grâce à la librairie ``re`` nous allons filtrer de manière à ne récupérer que la balise a qui contiendra en référence une adresse comportant le Nom de Domaine.\n",
    "\n",
    "Ce contenu sera stocké dans la variable `linkP` qui sera renvoyée comme résultat.\n",
    "\n",
    "Entre chaque lancement de la fonction, nous aurons un `time. sleep` qui évitera un potentiel blacklisting de notre script.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#************************[ Fonctions Utilisées ]************************************************************************************\n",
    "\n",
    "def Scrapper(Referring,Cible):\n",
    "    req =''\n",
    "    soup =''\n",
    "    link =''\n",
    "    linkP=''\n",
    "    if Referring =='':\n",
    "        pass\n",
    "    else:\n",
    "        req = requests.get(Referring) # La variable req récupére l'adresse présente dans 'Referring Page URL'\n",
    "        soup = BeautifulSoup(req.text, 'lxml') #BeautifulSoup récupére la totalité de la page signalée au dessus\n",
    "        for link in soup.find_all('a', href=re.compile(Cible)): #la fonction recherche le lien renvoyant vers la page de PortCros sur la page stockée\n",
    "            linkP = link.parent #On récupére le contexte du lien présent sur la page\n",
    "        time.sleep(2) #On marque une pause pour éviter le blacklisting potentiel de l'adresse IP sur le site.\n",
    "    return linkP #On retourne le resultat dans DfVar['Scrapped']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prétraitement des données\n",
    "\n",
    "Cette fonction se basera sur le fichier `appended.csv` que nous avons créé lors de l’exécution de notre première fonction.\n",
    "\n",
    "Cependant, comme pour le géoréférencement, nous avons choisi de limiter notre recherche de contexte aux acteurs varois.\n",
    "De ce fait, nous allons également charger `DNS_Adresses_Filtrees.csv` que nous avons créé lors de la définition de notre géoréférencement.\n",
    "\n",
    "Grâce à ces deux fichiers, nous allons effectuer un \"Merge\" dans la nouvelle variable `DfVar`, qui sera paramétrée de manière à ne récupérer que le contenu commun aux deux fichiers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#************************[ Traitement Initial ]************************************************************************************\n",
    "Df = pd.read_csv('appended.csv', encoding='utf-16', sep='\\t') #Ouverture du csv contenant la totalité des adresses recensées\n",
    "DfFilter = pd.read_csv('DNS_Adresses_Filtrees.csv', encoding='utf-16', sep='\\t') #Ouverture du csv contenant les adresses varoises\n",
    "\n",
    "DfVar = pd.merge(Df, DfFilter, on='DNS Source', how='inner') #On fusionne les deux csv précédents en ne gardant que les adresses ayant un dns en commun\n",
    "\n",
    "print(DfVar.info(verbose=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous passerons donc d’un traitement de 286 696 références à un traitement de 41 119 références.\n",
    "Soit 1/7 ème du fichier initial.\n",
    "\n",
    "## Initialisation du script\n",
    "\n",
    "Nous allons donc initialiser le script à partir de la variable `DfVar`.\n",
    "Nous déterminons une nouvelle colonne dans les données qui sera nommée `Scrapped` et qui contiendra le retour de la fonction ``Scrapper``.\n",
    "Nous utilisons la fonction Pandas progress_apply dans laquelle nous allons déterminer les correspondances entre la variable et la fonction `DfVar`.\n",
    "La Colonne `Referring Page URL` correspondra à la variable referring de Scrapper et DNS Cible à la partie Cible de Scrapper.\n",
    "\n",
    "De plus grâce à `progress_apply` nous pourrons voir l’évolution de la progression du traitement des données ainsi qu’une estimation du temps restant avant la fin de ce dernier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#************************[  ]************************************************************************************\n",
    "\n",
    "DfVar['Scrapped'] =DfVar.progress_apply(lambda x : Scrapper(x['Referring Page URL'],x ['DNS Cible']),axis=1) #On lance la fonction Scrapper en se basant sur les Colonnes énumérées : le résultat arrive dans \"Scrapped\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour finir, nous restituerons les données dans un dernier fichier Csv nommé `PortCrosScrapped.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#************************[ Restitution des données ]************************************************************************************\n",
    "\n",
    "DfVar.to_csv('PortCrosScrapped.csv', encoding=\"utf-16\", sep='\\t') #Fichier de sortie comportant l'ensemble des données récupérées entre les différentes étapes pour les adresses varoises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-0ea39e66e148>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-0ea39e66e148>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    Afin de détailler précisément les informations que nous avons, nous souhaitons connaitre le contexte d'utilisation de l'hyperlien vers le site du Parc National.\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "%reset -f"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
