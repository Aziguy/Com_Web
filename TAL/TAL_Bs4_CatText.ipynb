{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "##### Mes librairies\n",
    "Bien vouloir exécuter cette ligne en première _\n",
    "\n",
    "Cela prendra un peu de temps (le temps de charger les différents modèles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > installPackage.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\StageDASI2020\\lib\\site-packages\\tqdm\\std.py:668: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "import urllib.request\n",
    "import spacy, requests\n",
    "from spacy import displacy\n",
    "from spacy_cld import LanguageDetector\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stopWords = stopwords.words('french')\n",
    "nlp = spacy.load('fr_core_news_sm') # On charge le modèle français\n",
    "# nlp = spacy.load('en_core_web_sm') # On charge le modèle anglais\n",
    "nlp2 = spacy.load('en_core_web_lg') # On charge le modèle le plus large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `tag_visible()`\n",
    "* `text_from_html()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def text_from_html(body):\n",
    "    soup = BeautifulSoup(body, 'html.parser')\n",
    "    texts = soup.findAll(text=True)\n",
    "    visible_texts = filter(tag_visible, texts)  \n",
    "    return u\" \".join(t.strip() for t in visible_texts)\n",
    "\n",
    "# Cette méthode permet de tokenizer mon texte\n",
    "# Elle retourne une liste\n",
    "\n",
    "def return_token(texte):\n",
    "    # Tokeniser la phrase\n",
    "    mod = nlp(texte)\n",
    "    # Retourner le texte de chaque token\n",
    "    return [X.text for X in mod]\n",
    "\n",
    "# Cette méthode permet de supprimer les stopWords dans mon texte\n",
    "# Elle retourne une liste\n",
    "\n",
    "def cleanWords(liste):\n",
    "    clean_words = []\n",
    "    for token in liste:\n",
    "        if token not in stopWords:\n",
    "            clean_words.append(token)\n",
    "    return clean_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Cette méthode ``wordExtractor`` permet de recenser les mots les plus fréquents dans un texte récupéré via le scraping\n",
    "Note: Il aurait mieux vallu factoriser ces deux méthodes : une méthode qui extrait du contenu d'un texte. \n",
    "Une méthode qui lit un fichier csv (\"cat;url;\\n\") pour aller extraire le texte de l'URL et ranger dans cat à l'aide la méthode précédente.\n",
    "\n",
    "Nous allons utiliser cette méthode de deux manière différente (son implémentation sera aussi différente bien sûr)\n",
    "* wordExtrator(fichier) : \n",
    "\n",
    "Elle prend en paramètre un fichier excel et retourne un dictionnaire ayant pour clé le type de site et pour valeur la liste des mots  du type \n",
    "\n",
    "* wordExtrator(lien) :\n",
    "\n",
    "Elle prend en paramètre un lien (url) et retourne les vingt (20) mots les plus fréquents dans la page d'acceuil du lien.\n",
    "###### Cette méthode `tupleToString` permet de convertir un tuple en chaine de caractère\n",
    "Elle prend en  paramètre un tuple et retourne une chaine (string) sui sera facilement stokable dans un dataframe. A l'aide de Spacy (variable `nlp`ou `nlp2`) on pourra facilement itérer sur chaque mot de la chaine de caractère."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tupleToString(monTuple):\n",
    "    \n",
    "    maListe = []\n",
    "    maChaine = \"\"\n",
    "    for element in monTuple:\n",
    "        temp = element[0]\n",
    "        maListe.append(temp)\n",
    "    maChaine = \" \".join(str(x) for x in maListe)\n",
    "    \n",
    "    return maChaine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implémentation de `wordExtrator(fichier)`\n",
    "\n",
    "def wordExtractor(fichier):\n",
    "    # Mes variables\n",
    "    dicoMotsType = dict()\n",
    "    common_words = tuple()\n",
    "    lien, cat = \"\", \"\"\n",
    "    # Lecture de mon fichier Excel\n",
    "    data = pd.read_excel(fichier, \"siteType\", usecols = \"A:C\")\n",
    "    for indice, ligne in data.iterrows():\n",
    "        cat = ligne['Catégories']\n",
    "        lien = ligne['URL Acteurs']\n",
    "        \n",
    "        try:\n",
    "            #reponse = requests.get(\"http://{}\".format(lien))\n",
    "            html = urllib.request.urlopen(\"http://{0}\".format(lien)).read()\n",
    "            monTexte = text_from_html(html)\n",
    "            # NLP : Natural Language Processing\n",
    "            doc = nlp(monTexte)\n",
    "            allWords = [token.text for token in doc if token.is_stop != True and token.is_punct != True and token.is_space != True and token.is_digit != True and token.is_ascii and token.is_bracket != True and token.is_currency != True and len(token) > 1]\n",
    "            # 20 mots les plus fréquents\n",
    "            word_freq = cleanWords(allWords)\n",
    "            word_freq_count = Counter(word_freq)\n",
    "            common_words = word_freq_count.most_common(20) # common_words contient ici un tuple(mot, nbre occurence)\n",
    "            \n",
    "            for mot in common_words:\n",
    "                if cat not in dicoMotsType:\n",
    "                    dicoMotsType[cat] = []\n",
    "                    dicoMotsType[cat].append(mot[0])\n",
    "                else:\n",
    "                    dicoMotsType[cat].append(mot[0])   \n",
    "        except:\n",
    "            common_wordsm = None\n",
    "    return dicoMotsType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implémentation de `wordExtrator(lien)`\n",
    "def wordExtractor(lien):\n",
    "    \n",
    "    # Mes variables\n",
    "    common_words = tuple()\n",
    "    maChaine = \"\"\n",
    "    try:\n",
    "        #reponse = requests.get(\"http://{}\".format(lien))\n",
    "        html = urllib.request.urlopen(\"http://{0}\".format(lien)).read()\n",
    "        monTexte = text_from_html(html)\n",
    "        # NLP : Natural Language Processing\n",
    "        doc = nlp(monTexte)\n",
    "        allWords = [token.text for token in doc if token.is_stop != True and token.is_punct != True and token.is_space != True and token.is_digit != True and token.is_ascii and token.is_bracket != True and token.is_currency != True and len(token) > 1]\n",
    "        # 20 mots les plus fréquents\n",
    "        word_freq = cleanWords(allWords)\n",
    "        word_freq_count = Counter(word_freq)\n",
    "        common_words = word_freq_count.most_common(20) # common_words contient ici un tuple(mot, nbre occurence)   \n",
    "    except:\n",
    "        common_wordsm = None\n",
    "    # transformation de mon tuple en String\n",
    "    maChaine = tupleToString(common_words)\n",
    "    \n",
    "    return maChaine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Nettoyage du dictionnaire retourné par `wordExtractor(fichier)` à l'aide de la méthode ``cleanDictionnaire(monDico)``\n",
    "Elle nous retourne un nouveau dictionnaire `dicoMotTypeClean` qui contient la liste unique de mots représentant un type de site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanDictionnaire(monDico):\n",
    "    dicoMotsType = monDico\n",
    "    dicoMotTypeClean = dict()\n",
    "    \n",
    "    for cle, valeur in dicoMotsType.items():\n",
    "        for val in valeur:\n",
    "            dicoMotTypeClean[cle] = set(valeur)\n",
    "            \n",
    "    return dicoMotTypeClean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Ce script permet de dectecter la langue d'un texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('fr_core_news_sm') # On charge le disctionnaire français\n",
    "langDetect = LanguageDetector()\n",
    "nlp.add_pipe(langDetect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Test du script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ce texte a été rédigé en français\n"
     ]
    }
   ],
   "source": [
    "texte = nlp(\"Ceci est un texte en français\")\n",
    "reponse = texte._.languages\n",
    "for i in reponse:\n",
    "    if i == 'fr':\n",
    "        print(\"Ce texte a été rédigé en français\")\n",
    "    else:\n",
    "        print(\"Ce texte a été rédigé dans une langue inconnue!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Test de ma méthode `wordExtrator(lien)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Camping Le Les vacances savoir windsurf kitesurf Spa Porquerolles Giens camping place mobil-home International Aphrodite Or vue mer alimentation Locations'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aff = wordExtractor(\"campingshyeres.com\")\n",
    "aff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Test de ma méthode `wordExtrator(fichier)` et nettoyage du dictionnaire obtenu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-c14202ee6c94>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmonFichier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"WordsExtract.xlsx\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdictionnaire\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwordExtractor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmonFichier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdico_Ok\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcleanDictionnaire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdictionnaire\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-4c2120424a68>\u001b[0m in \u001b[0;36mcleanDictionnaire\u001b[1;34m(monDico)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mdicoMotTypeClean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mcle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvaleur\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdicoMotsType\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mval\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvaleur\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m             \u001b[0mdicoMotTypeClean\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcle\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvaleur\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "monFichier = \"WordsExtract.xlsx\"\n",
    "dictionnaire = wordExtractor(monFichier)\n",
    "dico_Ok = cleanDictionnaire(dictionnaire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"après : \")\n",
    "print([len(dico_Ok[x]) for x in dico_Ok])\n",
    "print(\"Avabt : \")\n",
    "print([len(dictionnaire[x]) for x in dictionnaire])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def return_word_embedding(sentence):\n",
    "    # Tokeniser la phrase\n",
    "    doc = nlp(sentence)\n",
    "    # Retourner le vecteur lié à chaque token\n",
    "    return [(mot.vector) for mot in doc]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_word_embedding(monTexte)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Je teste la similarité entre deux chaines de même longueur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp2(\"Puis-je comment avoir un animal domestique comment ?\")\n",
    "tab = ['Comment', 'puis-je', 'obtenir', 'un', 'animal', 'de', 'compagnie', '?']\n",
    "maChaine = \" \".join(str(x) for x in tab)\n",
    "doc2 = nlp2(maChaine)\n",
    "doc1.similarity(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabComp = [(wd1.text, wd2.text, wd1.similarity(wd2)) for wd2 in doc2 for wd1 in doc1]\n",
    "conversion = np.array(tabComp[0][2])\n",
    "print(conversion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monTexte1 = \"Tout est possible en python\"\n",
    "monTexte2 = \"Presque tout est possible en python\"\n",
    "doc1 = nlp(monTexte1)\n",
    "doc2 = nlp(monTexte2)\n",
    "doc1.similarity(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp2(\"Aix-en-provence chambres Aix provence centre hotel HOTEL AIX PROVENCE ville Alentours Tarifs Contact Internet chambre Aix-en-Provence Sainte pays FAX hotelvendome13@orange.fr\")\n",
    "doc2 = nlp2(\"Brasserie Bar Soleil Rascasse vue restaurant Barbecue conditions Chambres giens mer Parc restaurants hotel var pers J-14 Restaurant Tourisme panoramique\")\n",
    "doc1.similarity(doc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Scraping gestion de dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('zoneDNS.csv', encoding='utf-16', sep='\\t') # Lecture du fichier d'adresses\n",
    "data['Mots Fréquents'] = data['DNS Source'].progress_apply(lambda x : wordExtractor(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('sortieWords.csv', encoding='utf-16', index=True, header=True, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(spacy.tokens.token.Token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('we love provence 12 .. !')\n",
    "\n",
    "for token in doc:\n",
    "    if token.is_stop != True and token.is_punct != True and token.is_space != True and token.is_digit != True:\n",
    "        print(token.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Test sur geocode : avec le nom exacte d'une entité, on peut obtenir son type (université, restaurant, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les librairies\n",
    "import folium\n",
    "import pandas as pd\n",
    "import csv, os\n",
    "from geopy.geocoders import Nominatim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nom = Nominatim(user_agent=\"my-application\")\n",
    "n = nom.geocode(\"Var Tourisme\")\n",
    "print(dir(n))\n",
    "print(n.raw)\n",
    "print(n.latitude, n.longitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Nominatim(user_agent=\"my-application\")\n",
    "test1 = test.reverse(\"43.536419992209794, 6.460199997692357\")\n",
    "print(test1)\n",
    "print(test1.raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Test geocode via googlemaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import googlemaps\n",
    "g_mapsKey = googlemaps.Client(key=\"AIzaSyDJg3MbfuA50V6pPR4xI-GLhEfABxxAulk\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultat = g_mapsKey.geocode(\"www.aqualike.com\")\n",
    "print(resultat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Laboratoire d'essai `lab Test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel(\"WordsExtract.xlsx\", \"siteType\", usecols = \"A:C\")\n",
    "#df = df.set_index(\"Catégories\")\n",
    "siteType = list(set(df.Catégories))\n",
    "print(siteType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['Catégories'] == \"Hôtel\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monDictMots = dict()\n",
    "for index, row in df.iterrows():\n",
    "    if row['Catégories'] in monDictMots:\n",
    "        #if row['Catégories'] not in monDict.keys():\n",
    "            #monDict[row['Catégories']].append(row['URL Acteurs'])\n",
    "        #else:\n",
    "        monDictMots[row['Catégories']].append(row['Mots'])\n",
    "    else:\n",
    "        monDictMots[row['Catégories']] = []\n",
    "        monDictMots[row['Catégories']].append(row['Mots'])\n",
    "    #print(row['Catégories'], row['URL Acteurs'])\n",
    "print(monDictMots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monDict = dict()\n",
    "for index, row in df.iterrows():\n",
    "    if (row['Catégories'] in monDict):\n",
    "        #if row['Catégories'] not in monDict.keys():\n",
    "            #monDict[row['Catégories']].append(row['URL Acteurs'])\n",
    "        #else:\n",
    "        monDict[row['Catégories']].append(row['URL Acteurs'])\n",
    "    else:\n",
    "        monDict[row['Catégories']] = []\n",
    "        monDict[row['Catégories']].append(row['URL Acteurs'])\n",
    "    #print(row['Catégories'], row['URL Acteurs'])\n",
    "print(monDict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,value in monDict.items():\n",
    "    print(f'\\n {key}:\\n')\n",
    "    for valeur in value:\n",
    "        print(valeur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monFichier = \"WordsExtract.xlsx\"\n",
    "dicoClean = wordExtractor(monFichier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabMoy = []\n",
    "for wd1 in doc1:\n",
    "    for wd2 in doc2:\n",
    "        (wd1.text, wd2.text), \"similarité : \", wd1.similarity(wd2)\n",
    "        temp = wd1.similarity(wd2)\n",
    "        tabMoy.append(temp)\n",
    "print(np.array(tabMoy).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
